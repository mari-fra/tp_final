{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa56bd82",
   "metadata": {},
   "source": [
    "\n",
    "### FastText Embeddings & Neural Network\n",
    "\n",
    "El modelo FastText permite el enriquecimiento de vectores de palabras con informaci√≥n de subpalabras.\n",
    "\n",
    "fastText trata cada palabra como la agregaci√≥n de sus subpalabras. Las subpalabras se definen como los n-gramas de caracteres de la palabra. El vector de una palabra se calcula simplemente como la suma de todos los vectores de sus n-gramas de caracteres componentes.\n",
    "\n",
    "fastText puede obtener vectores incluso para palabras fuera del vocabulario (OOV), sumando los vectores de sus char-ngrams componentes, siempre que al menos uno de los char-ngrams estuviera presente en los datos de entrenamiento.\n",
    "\n",
    "Se implementaron los siguientes pasos:\n",
    "\n",
    "#### 1. Preprocesamiento y Creaci√≥n del corpus de palabras.\n",
    "\n",
    "Se eliminan acentos, caracteres especiales, espacios en blanco, se normalizan numeros (12 -> '00') y se obtine una lista unica de palabras (Corpus).\n",
    "\n",
    "#### 2. Generacion de embeddigns con FastText:\n",
    "\n",
    "Se entrena el modelo FastText con el Corpus para obtener los embeddings por cada token:\n",
    "\n",
    "`vectors_vocab.npy` almacena **one vector per token** (word). Las columnas corresponden a las dimensiones del vector (200 en este caso).\n",
    "\n",
    "`vectors_ngrams` **un vector para cada n-grama** (subword) en todas los tokens del vocabulario. Cada fila es un vector que corresponde a un grupo. Las columnas corresponden a las dimensiones del vector.\n",
    "\n",
    "FastText representa:\n",
    "\n",
    "* `\"celular\"` ‚Üí its own vector\n",
    "* `\"samsung\"` ‚Üí its own vector\n",
    "* `\"galaxy\"` ‚Üí its own vector\n",
    "* y tambi√©n sub-words `\"celu\"`, `\"ular\"`, `\"sams\"`, `\"galax\"`, etc.\n",
    "\n",
    "El vocabulario tiene word y subwords:\n",
    "\n",
    "```\n",
    "[\"celular\", \"samsung\", \"galaxy\", \"celu\", \"ular\", \"galax\", ...]\n",
    "```\n",
    "\n",
    "\n",
    "#### 4. Tokenizaci√≥n\n",
    "\n",
    "Se utiliza el tokenizador de keras que convierte cada token en un ID. Y a una sentence en una lista de IDs\n",
    "\n",
    "```python\n",
    "sentence = \"celular samsung galaxy\"\n",
    "```\n",
    "\n",
    "y el tokenizar contruye este mapeo:\n",
    "\n",
    "```python\n",
    "tokenizer.word_index = {\n",
    "    \"celular\": 1,\n",
    "    \"samsung\": 2,\n",
    "    \"galaxy\": 3\n",
    "}\n",
    "```\n",
    "\n",
    "Luego `tokenizer.texts_to_sequences([\"celular samsung galaxy\"])` ‚Üí `[[1, 2, 3]]`\n",
    "\n",
    "#### 5. Construction de `embedding_matrix`\n",
    "\n",
    "El objetivo es **alinear** el vector con el **indices del tokenizedor**, para que el modelo de ML pueda mapear *word IDs ‚Üí pretrained embeddings.*\n",
    "\n",
    "```python\n",
    "for word, idx in tokenizer.word_index.items():\n",
    "    if word in fasttext_model.wv:\n",
    "        embedding_matrix[idx] = fasttext_model.wv[word]\n",
    "```\n",
    "\n",
    "Embedding lookup: \n",
    "\n",
    "Cuando introducimos la secuencia en el modelo, el modelo busca:\n",
    "\n",
    "```\n",
    "embedding_matrix[1] ‚Üí vector for \"celular\"\n",
    "embedding_matrix[2] ‚Üí vector for \"samsung\"\n",
    "embedding_matrix[3] ‚Üí vector for \"galaxy\"\n",
    "```\n",
    "Se ubica por fila\n",
    "```\n",
    "[[v_celular],\n",
    " [v_samsung],\n",
    " [v_galaxy]]\n",
    "```\n",
    "\n",
    "Se obtiene un **2D array:** (3 words) √ó (200 embedding dimensions)\n",
    "\n",
    "#### 6. Entrenamiento de la Red Neuronal \n",
    "\n",
    "Se utilizan los embeddings que entrenamos previamente con FastText como base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c592cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import unidecode\n",
    "import spacy\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from joblib import dump, load\n",
    "import tqdm\n",
    "import gc\n",
    "import os\n",
    "import re\n",
    "\n",
    "def clean_spanish_text(text):\n",
    "    \"\"\"Limpieza b√°sica para espa√±ol: min√∫sculas, sin tildes, sin caracteres raros.\"\"\"\n",
    "    text = str(text).lower().strip()\n",
    "    text = unidecode.unidecode(text)  # remove accents\n",
    "    text = re.sub(r'\\d', '0', text)    # normalize numbers\n",
    "    text = re.sub(r'[^a-z0-9√± ]', ' ', text)  # keep only letters, numbers, √±\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # remove extra spaces\n",
    "    return text\n",
    "\n",
    "def build_corpus_from_dataframe(df):\n",
    "    \"\"\"Crea una lista de listas de palabras a partir del dataframe (columna 'text').\"\"\"\n",
    "    df['text'] = df['text'].apply(clean_spanish_text)\n",
    "    return df['text'].str.split().tolist()\n",
    "\n",
    "def train_fasttext_es(corpus, output_dir=\"models_fasttext_es\", \n",
    "                      vector_size=200, window=8, min_count=5, workers = 4):\n",
    "    \"\"\"Entrena y guarda un modelo FastText en espa√±ol.\"\"\"\n",
    "    print(\"üß† Entrenando modelo FastText (es)...\")\n",
    "    model = gensim.models.FastText(\n",
    "        sentences=corpus,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        sg=1,\n",
    "        workers=workers\n",
    "    )\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model.save(os.path.join(output_dir, \"fasttext_es.bin\"))\n",
    "    print(\"‚úÖ Modelo FastText guardado en:\", output_dir)\n",
    "    return model\n",
    "\n",
    "def build_embedding_matrix_es(model_path, tokenizer_path, output_dir=\"models_fasttext_2_es\"):\n",
    "    \"\"\"Crea la matriz de embeddings alineada con el tokenizer (solo espa√±ol).\"\"\"\n",
    "    print(\"‚öôÔ∏è Construyendo matriz de embeddings (es)...\")\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, \"embedding_matrix_es.npy\")\n",
    "\n",
    "    # Si ya existe un embedding_matrix guardado, lo reutiliza\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"üîÅ Archivo existente encontrado: {output_path}\")\n",
    "        print(\"Cargando matriz desde disco...\")\n",
    "        return np.load(output_path)\n",
    "\n",
    "    # Carga modelo FastText y tokenizer\n",
    "    model = gensim.models.FastText.load(model_path)\n",
    "    word_vectors = model.wv\n",
    "    tokenizer = load(tokenizer_path)\n",
    "\n",
    "    nlp = spacy.load(\"es_core_news_sm\", disable=['parser', 'ner'])\n",
    "    stemmer = SnowballStemmer(language='spanish')\n",
    "\n",
    "    # Inicializa la matriz (una fila por palabra del tokenizer x vector size (200))\n",
    "    embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, word_vectors.vector_size))\n",
    "    unknown = 0\n",
    "\n",
    "    # Construye los embeddings\n",
    "    for word, idx in tqdm.tqdm(tokenizer.word_index.items()):\n",
    "        vector = None\n",
    "        # Crea un FastText vector por cada word in keras.tokenizer.\n",
    "        # Si la word no existe en los word_vectors, busca por sus variantes:\n",
    "        #   > unidecode(), lemma_ or stemmer\n",
    "        # Ante el primer match entre word/variante & word_vectors, se sale del bucle\n",
    "        for variant in [word, unidecode.unidecode(word), nlp(word)[0].lemma_, stemmer.stem(word)]:\n",
    "            if variant in word_vectors:\n",
    "                vector = word_vectors[variant]\n",
    "                break\n",
    "        # agrega el vector a la matriz\n",
    "        if vector is not None:\n",
    "            embedding_matrix[idx] = vector\n",
    "        else:\n",
    "            unknown += 1\n",
    "\n",
    "    print(f\"üîç Palabras desconocidas: {unknown}\")\n",
    "    print(f\"üíæ Guardando matriz en {output_path} ...\")\n",
    "\n",
    "    np.save(output_path, embedding_matrix)\n",
    "    print(\"‚úÖ Embedding matrix guardada correctamente.\")\n",
    "    \n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c81ef7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1be87ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>label_quality</th>\n",
       "      <th>language</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hidrolavadora Lavor One 120 Bar 1700w  Bomba A...</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>spanish</td>\n",
       "      <td>ELECTRIC_PRESSURE_WASHERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Placa De Sonido - Behringer Umc22</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>spanish</td>\n",
       "      <td>SOUND_CARDS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Maquina De Lavar Electrolux 12 Kilos</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>portuguese</td>\n",
       "      <td>WASHING_MACHINES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Par Disco De Freio Diant Vent Gol 8v 08/ Frema...</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>portuguese</td>\n",
       "      <td>VEHICLE_BRAKE_DISCS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Flashes Led Pesta√±as Luminoso Falso Pesta√±as P...</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>spanish</td>\n",
       "      <td>FALSE_EYELASHES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title label_quality  \\\n",
       "0  Hidrolavadora Lavor One 120 Bar 1700w  Bomba A...    unreliable   \n",
       "1                  Placa De Sonido - Behringer Umc22    unreliable   \n",
       "2               Maquina De Lavar Electrolux 12 Kilos    unreliable   \n",
       "3  Par Disco De Freio Diant Vent Gol 8v 08/ Frema...    unreliable   \n",
       "4  Flashes Led Pesta√±as Luminoso Falso Pesta√±as P...    unreliable   \n",
       "\n",
       "     language                   category  \n",
       "0     spanish  ELECTRIC_PRESSURE_WASHERS  \n",
       "1     spanish                SOUND_CARDS  \n",
       "2  portuguese           WASHING_MACHINES  \n",
       "3  portuguese        VEHICLE_BRAKE_DISCS  \n",
       "4     spanish            FALSE_EYELASHES  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41360d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_all[df_all['language'] == 'spanish']\n",
    "df_sample = df_sample.rename(columns = {'title': 'text', 'category' : 'labels'})\n",
    "df_sample = df_sample[['text', 'labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fd2441c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hidrolavadora Lavor One 120 Bar 1700w  Bomba A...</td>\n",
       "      <td>ELECTRIC_PRESSURE_WASHERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Placa De Sonido - Behringer Umc22</td>\n",
       "      <td>SOUND_CARDS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Flashes Led Pesta√±as Luminoso Falso Pesta√±as P...</td>\n",
       "      <td>FALSE_EYELASHES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Gatito Lunchera Neoprene</td>\n",
       "      <td>LUNCHBOXES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Rosario Contador De Billetes Uv / Mg Detecta F...</td>\n",
       "      <td>BILL_COUNTERS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  \\\n",
       "0   Hidrolavadora Lavor One 120 Bar 1700w  Bomba A...   \n",
       "1                   Placa De Sonido - Behringer Umc22   \n",
       "4   Flashes Led Pesta√±as Luminoso Falso Pesta√±as P...   \n",
       "9                           Gatito Lunchera Neoprene    \n",
       "11  Rosario Contador De Billetes Uv / Mg Detecta F...   \n",
       "\n",
       "                       labels  \n",
       "0   ELECTRIC_PRESSURE_WASHERS  \n",
       "1                 SOUND_CARDS  \n",
       "4             FALSE_EYELASHES  \n",
       "9                  LUNCHBOXES  \n",
       "11              BILL_COUNTERS  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c3f93b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf38cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_samples = 20\n",
    "# valid_labels = df_sample['labels'].value_counts()\n",
    "# valid_labels = valid_labels[valid_labels >= min_samples].index\n",
    "# df_sample = df_sample[df_sample['labels'].isin(valid_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05e47835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hidrolavadora Lavor One 120 Bar 1700w  Bomba A...</td>\n",
       "      <td>ELECTRIC_PRESSURE_WASHERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Placa De Sonido - Behringer Umc22</td>\n",
       "      <td>SOUND_CARDS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Hidrolavadora Lavor One 120 Bar 1700w  Bomba A...   \n",
       "1                  Placa De Sonido - Behringer Umc22   \n",
       "\n",
       "                      labels  \n",
       "0  ELECTRIC_PRESSURE_WASHERS  \n",
       "1                SOUND_CARDS  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.iloc[[0,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33d9b2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maribel.fraire\\AppData\\Local\\Temp\\ipykernel_10532\\7008074.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text'] = df['text'].apply(clean_spanish_text)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['hidrolavadora',\n",
       "  'lavor',\n",
       "  'one',\n",
       "  '000',\n",
       "  'bar',\n",
       "  '0000w',\n",
       "  'bomba',\n",
       "  'aluminio',\n",
       "  'italia'],\n",
       " ['placa', 'de', 'sonido', 'behringer', 'umc00']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample_filter = df_sample.iloc[[0,1]]\n",
    "build_corpus_from_dataframe(df_sample_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d0fde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£ Crear corpus\n",
    "# Apply clean_spanish_text to df_sample['text'] and create corpus\n",
    "corpus = build_corpus_from_dataframe(df_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ad28d8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fa7eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Entrenando modelo FastText (es)...\n",
      "‚úÖ Modelo FastText guardado en: models_fasttext_2_es\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gensim.models.fasttext.FastText at 0x216f7733eb0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2Ô∏è‚É£ Entrenar modelo FastText espa√±ol\n",
    "fasttext_model = train_fasttext_es(corpus, output_dir=\"models_fasttext_2_es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1e396cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "fasttext_model = FastText.load(\"models_fasttext_2_es/fasttext_es.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71340a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models_fasttext_2_es/tokenizer_es.dump']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3Ô∏è‚É£ Crear y guardar tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# df_sample['text'] is already cleaned at this point\n",
    "tokenizer.fit_on_texts(df_sample['text'])\n",
    "dump(tokenizer, \"models_fasttext_2_es/tokenizer_es.dump\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22034085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in tokenizer: 658651\n",
      "0: 1\n",
      "de: 2\n",
      "00: 3\n",
      "000: 4\n",
      "0000: 5\n",
      "para: 6\n",
      "x: 7\n",
      "con: 8\n",
      "y: 9\n",
      "a: 10\n"
     ]
    }
   ],
   "source": [
    "print(\"Total words in tokenizer:\", len(tokenizer.word_index))\n",
    "for word, idx in list(tokenizer.word_index.items())[:10]:\n",
    "    print(f'{word}: {idx}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b236e5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Construyendo matriz de embeddings (es)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 658651/658651 [19:40<00:00, 557.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Palabras desconocidas: 0\n",
      "üíæ Guardando matriz en models_fasttext_2_es\\embedding_matrix_es.npy ...\n",
      "‚úÖ Embedding matrix guardada correctamente.\n"
     ]
    }
   ],
   "source": [
    "# 4Ô∏è‚É£ Generar embedding matrix \n",
    "embedding_matrix = build_embedding_matrix_es(model_path = \"models_fasttext_2_es/fasttext_es.bin\", \n",
    "                                             tokenizer_path= \"models_fasttext_2_es/tokenizer_es.dump\",\n",
    "                                             output_dir = \"models_fasttext_2_es\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7a3efa87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id_celular: 506\n",
      "[-0.02472354  0.09676356  0.13969924 -0.05794974 -0.06126994  0.11244962\n",
      " -0.08809794  0.1878252  -0.06249603 -0.19332647]\n",
      "len: 200\n"
     ]
    }
   ],
   "source": [
    "#Get id for celular\n",
    "id_celular = tokenizer.word_index['celular']\n",
    "print(f'id_celular: {id_celular}')\n",
    "\n",
    "#Get embedding for celular\n",
    "print(embedding_matrix[id_celular][:10])\n",
    "print(f'len: {len(embedding_matrix[id_celular])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "890d7233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(658652, 200)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f4ef0693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels\n",
       "BOOKS                          19010\n",
       "ACTION_FIGURES                 18433\n",
       "MAGAZINES                      18081\n",
       "DIECAST_VEHICLES               17923\n",
       "FOOTBALL_SHIRTS                17923\n",
       "                               ...  \n",
       "SCALE_RULERS                      49\n",
       "COMMERCIAL_POPCORN_MACHINES       36\n",
       "SNACK_HOLDERS                      9\n",
       "ANTI_STATIC_PLIERS                 5\n",
       "CARD_PAYMENT_TERMINALS             2\n",
       "Name: count, Length: 1574, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7552da68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "se vende galaxy tab0 0 0 wi fi\n",
      "tablet avh g00 funda con teclado\n",
      "ipad mini 0 retina wifi 00gb inmaculado\n",
      "ipad a 0000 0 00 gb wifi con funda lapiz y cable de datos\n",
      "samsung galaxy tab s0 0 0 00gb igual a nueva\n",
      "ipad air 0 00gb como nuevo con funda y accesorios\n",
      "tablet samsung note 00 0 wifi\n",
      "tablet philco tp0a0 0 kids blanco 0 gb funda naranja\n",
      "aire 0 caso robusta pata de cabra serie a prueba de\n",
      "tablet 0 0 pulgadas lenovo\n"
     ]
    }
   ],
   "source": [
    "df_tablets = df_sample['text'][df_sample['labels'] == 'TABLETS']\n",
    "for i in range(10):\n",
    "    print(df_tablets.iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f25832d",
   "metadata": {},
   "source": [
    "### Word vector lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ebbe82",
   "metadata": {},
   "source": [
    "Toda la informaci√≥n necesaria para buscar palabras fastText (incl. OOV words) se encuentra en su atributo `model.wv.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d209a701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('moto0', 0.7987481951713562),\n",
       " ('motoe0', 0.7944773435592651),\n",
       " ('motox', 0.7801886200904846),\n",
       " ('moto00', 0.7801306247711182),\n",
       " ('motoxwilde', 0.7686169743537903)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model.wv.most_similar(\"moto\", topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea506d6d",
   "metadata": {},
   "source": [
    "Previamente, utilic√© estos hyperparameters para entrenar los embeddings en models.FastText() \n",
    "\n",
    "vector_size=200, window=5, min_count=2\n",
    "\n",
    "Mi objetivo era que esto fasttext_model.wv.most_similar_cosmul( positive=[\"galaxy\"], negative=[\"apple\"], topn=5 ) retorne \"samsung\", sin embargo este fue el resultado: \n",
    "\n",
    "```PYTHON\n",
    "[('63', 1.0004220008850098),\n",
    "('3v', 1.0004171133041382),\n",
    "('rw', 1.0003960132598877),\n",
    "('84', 1.0002875328063965),\n",
    "('h/', 1.00026273727417)]\n",
    "```\n",
    "\n",
    "Parece el los numeros o junk son dominantes y Se excedi√≥ en la generaci√≥n de Character n-grams \n",
    "\n",
    "Al cambiar los hiperparametros a `window = 8` y `min_count = 5` el resultado tiene mas sentido. Al aumentar el min_count de 2 a 5, hace que se ignoren esos valores 'raros' (numeros, ngramas) ya que ahora le pido que tengan al menos 5 apariciones.\n",
    "\n",
    "- window: Context window size (Default 5)\n",
    "- min_count: Ignore words with number of occurrences below this (Default 5)\n",
    "\n",
    "\n",
    "[Doc](https://radimrehurek.com/gensim/auto_examples/tutorials/run_fasttext.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4507c3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('galax', 0.833404541015625), ('galaxi', 0.8131084442138672), ('galaxis', 0.7421173453330994), ('galaxie', 0.6850820183753967), ('j0', 0.6780985593795776)]\n",
      "[('tablets', 0.8222858905792236), ('phablet', 0.7704592943191528), ('inchtablet', 0.7572232484817505), ('ablet', 0.7494990229606628), ('tableto', 0.7379324436187744)]\n",
      "[('applea', 0.8668965697288513), ('appletv', 0.8461295962333679), ('apples', 0.8112044930458069), ('applewatch', 0.7704615592956543), ('applecare', 0.7282817363739014)]\n"
     ]
    }
   ],
   "source": [
    "print(fasttext_model.wv.most_similar(\"galaxy\", topn=5))\n",
    "print(fasttext_model.wv.most_similar(\"tablet\", topn=5))\n",
    "print(fasttext_model.wv.most_similar(\"apple\", topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cf002be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('smsung', 0.9951120018959045),\n",
       " ('galaxy', 0.9924536347389221),\n",
       " ('samsumg', 0.9871987700462341)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model.wv.most_similar_cosmul(\n",
    "    positive=[\"tab0\", \"samsung\"],\n",
    "    negative=[\"lenovo\"],\n",
    "    topn=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bae40612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('s0msung', 1.0625591278076172),\n",
       " ('smsung', 1.0573632717132568),\n",
       " ('sanmsung', 1.0518909692764282)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model.wv.most_similar_cosmul(\n",
    "    positive=[\"tab0\", \"samsung\"],\n",
    "    negative=[\"apple\"],\n",
    "    topn=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "41cf920e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ryzen', 0.9222611784934998),\n",
       " ('ryzen0', 0.9118472337722778),\n",
       " ('fryzen', 0.8881929516792297)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model.wv.most_similar_cosmul(positive=['amd', 'i0'], negative=['intel'], topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf44253",
   "metadata": {},
   "source": [
    "Antes del preprocesamiento y la normalizacion de los numeros '2025' -> '0000' '8' -> '0' la embedding_matrix era de (1228979, 200) **Se redujo m√°s de la mitad**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "efcb6607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(658652, 200)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a3ce2e",
   "metadata": {},
   "source": [
    "keras.tokenizer transforma cada token en un ID\n",
    "\n",
    "tokenizer.texts_to_sequences transforma una secuencia de palabras en una sequencia de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8702acbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[506, 80]]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([\"Celular Samsung\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e06e498f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.02472354,  0.09676356,  0.13969924, -0.05794974, -0.06126994,\n",
       "         0.11244962, -0.08809794,  0.1878252 , -0.06249603, -0.19332647]),\n",
       " array([ 0.03803588,  0.31229076,  0.16416894,  0.47289488, -0.08413718,\n",
       "        -0.19058801, -0.40931073, -0.28712058, -0.35503694, -0.27083719]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vector for celular    vector for samsung\n",
    "embedding_matrix[506][:10], embedding_matrix[80][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a67aee",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809329fb",
   "metadata": {},
   "source": [
    "Encode labels. Convertir de STRING a ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540bb62d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ELECTRIC_PRESSURE_WASHERS</td>\n",
       "      <td>539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SOUND_CARDS</td>\n",
       "      <td>1304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FALSE_EYELASHES</td>\n",
       "      <td>599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LUNCHBOXES</td>\n",
       "      <td>906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BILL_COUNTERS</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       labels  label_id\n",
       "0   ELECTRIC_PRESSURE_WASHERS       539\n",
       "1                 SOUND_CARDS      1304\n",
       "4             FALSE_EYELASHES       599\n",
       "9                  LUNCHBOXES       906\n",
       "11              BILL_COUNTERS       182"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df_sample['label_id'] = le.fit_transform(df_sample['labels'])\n",
    "df_sample[['labels', 'label_id']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be353a15",
   "metadata": {},
   "source": [
    "#### Load Tokenizer and FastText Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e33b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# from gensim.models import FastText\n",
    "# # Load tokenizer\n",
    "# with open(\"models_fasttext_es/tokenizer_es.dump\", \"rb\") as f:\n",
    "#     tokenizer = pickle.load(f)\n",
    "# # Load FastText model\n",
    "# fasttext_model = FastText.load(\"models_fasttext_es/fasttext_es.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ab778165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X = tokenizer.texts_to_sequences(df_sample[\"text\"])\n",
    "X = pad_sequences(X, maxlen=None, padding='post')\n",
    "y = df_sample[\"label_id\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5ad9db4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  823, 34333,   344,     4,   357,   128,    51,    86,  2342,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [   94,     2,   531,  1772, 24769,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [24017,    15,  1623,  2976,  4998,  1623,     6,  4406,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5204035f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 539, 1304,  599]),\n",
       " 0    ELECTRIC_PRESSURE_WASHERS\n",
       " 1                  SOUND_CARDS\n",
       " 4              FALSE_EYELASHES\n",
       " Name: labels, dtype: object)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:3], df_sample['labels'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cd77c2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maribel.fraire\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = embedding_matrix.shape[1]\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=50,\n",
    "        trainable=False  # keep FastText vectors fixed\n",
    "    ),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=Adam(learning_rate=1e-3),\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6d2a589c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "944134e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m62500/62500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m509s\u001b[0m 8ms/step - accuracy: 0.6163 - loss: 1.7396 - val_accuracy: 0.8049 - val_loss: 0.8887\n",
      "Epoch 2/10\n",
      "\u001b[1m62500/62500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m540s\u001b[0m 9ms/step - accuracy: 0.6712 - loss: 1.4466 - val_accuracy: 0.8106 - val_loss: 0.8706\n",
      "Epoch 3/10\n",
      "\u001b[1m62500/62500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m581s\u001b[0m 9ms/step - accuracy: 0.6764 - loss: 1.4267 - val_accuracy: 0.8119 - val_loss: 0.8634\n",
      "Epoch 4/10\n",
      "\u001b[1m62500/62500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m641s\u001b[0m 10ms/step - accuracy: 0.6796 - loss: 1.4133 - val_accuracy: 0.8127 - val_loss: 0.8598\n",
      "Epoch 5/10\n",
      "\u001b[1m62500/62500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m637s\u001b[0m 10ms/step - accuracy: 0.6815 - loss: 1.4056 - val_accuracy: 0.8133 - val_loss: 0.8556\n",
      "Epoch 6/10\n",
      "\u001b[1m62500/62500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m644s\u001b[0m 10ms/step - accuracy: 0.6829 - loss: 1.3998 - val_accuracy: 0.8145 - val_loss: 0.8528\n",
      "Epoch 7/10\n",
      "\u001b[1m62500/62500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m627s\u001b[0m 10ms/step - accuracy: 0.6838 - loss: 1.3953 - val_accuracy: 0.8134 - val_loss: 0.8537\n",
      "Epoch 8/10\n",
      "\u001b[1m62500/62500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m625s\u001b[0m 10ms/step - accuracy: 0.6844 - loss: 1.3926 - val_accuracy: 0.8139 - val_loss: 0.8527\n",
      "Epoch 9/10\n",
      "\u001b[1m62500/62500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m408s\u001b[0m 7ms/step - accuracy: 0.6851 - loss: 1.3909 - val_accuracy: 0.8145 - val_loss: 0.8505\n",
      "Epoch 10/10\n",
      "\u001b[1m62500/62500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m390s\u001b[0m 6ms/step - accuracy: 0.6852 - loss: 1.3896 - val_accuracy: 0.8149 - val_loss: 0.8508\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f51ab8",
   "metadata": {},
   "source": [
    "La m√©trica de **accuracy en validaci√≥n es de 0.81**. \n",
    "\n",
    "Previamente, se prob√≥ el modelo en el dataset sin eliminar caracteres especiales/puntuaci√≥n y sin normalizar los numeros y se obtubo 0.79 de accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fc29836e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"models_fasttext_2_es/text_classifier_es.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a61afc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "# model = load_model(\"models_fasttext_2_es/text_classifier_es.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
